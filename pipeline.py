try:
    import pathway as pw
except ImportError:
    # Mock pathway for development
    class MockSchema:
        def __init__(self, **kwargs):
            for key, value in kwargs.items():
                setattr(self, key, value)
    
    class MockPathway:
        def __init__(self):
            self.Schema = MockSchema
            self.io = MockIO()
        
        def run(self):
            print("Mock Pathway pipeline running...")
            return "Mock pipeline completed"
    
    class MockIO:
        def __init__(self):
            self.fs = MockFS()
    
    class MockFS:
        def read(self, path, format, schema, mode):
            print(f"Mock Pathway: Would monitor {path} for {format} files")
            return None
    
    pw = MockPathway()

import asyncio
import json
import os
from typing import Dict, Any, Optional, List
from datetime import datetime
import requests

# Handle optional ML dependencies
try:
    from sentence_transformers import SentenceTransformer
    import numpy as np
    HAS_ML_DEPS = True
except ImportError:
    print("Warning: ML dependencies not available. Some features will be limited.")
    SentenceTransformer = None
    np = None
    HAS_ML_DEPS = False
from location_analyzer import LocationAnalyzer
from openai import OpenAI
from dotenv import load_dotenv

class LiveRAGPipeline:
    def __init__(self, data_dir: str = "./live_data_feed"):
        # Force reload environment variables
        load_dotenv(override=True)
        
        self.data_dir = data_dir
        self.openrouter_api_key = os.getenv("OPENROUTER_API_KEY")
        self.openrouter_base_url = os.getenv("OPENROUTER_BASE_URL", "https://openrouter.ai/api/v1")
        self.model_name = os.getenv("MODEL_NAME", "google/gemma-3n-e4b-it:free")
        self.base_cost = float(os.getenv("BASE_INSURANCE_COST", "500"))
        self.risk_multiplier = float(os.getenv("RISK_MULTIPLIER", "0.1"))
        
        # Initialize location analyzer
        self.location_analyzer = LocationAnalyzer()
        
        # Initialize OpenAI client for OpenRouter with robust validation
        self.openai_client = None
        if (self.openrouter_api_key and 
            self.openrouter_api_key != "your_openrouter_api_key_here" and
            len(self.openrouter_api_key.strip()) > 10):  # Basic key validation
            try:
                self.openai_client = OpenAI(
                    base_url=self.openrouter_base_url,
                    api_key=self.openrouter_api_key
                )
                print(f"✅ OpenRouter client initialized with model: {self.model_name}")
            except Exception as e:
                print(f"⚠️ Failed to initialize OpenRouter client: {e}")
                self.openai_client = None
        else:
            print("⚠️ OpenRouter API key not found or invalid - using simulation mode")
        
        # Initialize embedding model
        if HAS_ML_DEPS and SentenceTransformer:
            self.embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
        else:
            self.embedding_model = None
            print("Warning: Embedding model not available. Vector search disabled.")
        
        # Store for vector index
        self.documents = []
        self.embeddings = []
        self.document_metadata = []
        
        # Initialize Pathway pipeline
        self.setup_pathway_pipeline()
    
    def setup_pathway_pipeline(self):
        """Setup Pathway streaming pipeline"""
        try:
            # Use Pathway's filesystem connector to watch JSON files
            # First, ensure we have some initial data
            os.makedirs(self.data_dir, exist_ok=True)
            
            # Create an initial test file if directory is empty
            files_in_dir = [f for f in os.listdir(self.data_dir) if f.endswith('.json')]
            if not files_in_dir:
                initial_data = {
                    'source': 'system_init',
                    'timestamp': datetime.now().isoformat(),
                    'location': 'system',
                    'content': 'System initialized - monitoring for live data feeds',
                    'type': 'system'
                }
                with open(os.path.join(self.data_dir, 'init.json'), 'w') as f:
                    json.dump(initial_data, f, indent=2)
            
            # Define schema for our data
            class InputSchema(pw.Schema):
                source: str
                timestamp: str
                location: str
                content: str
                type: str
            
            # Use Pathway's filesystem connector for streaming JSON files
            self.input_table = pw.io.fs.read(
                path=self.data_dir,
                format="json",
                schema=InputSchema,
                mode="streaming"
            )
            
            # Process the data stream
            if self.input_table is not None:
                # Apply real-time transformations
                self.processed_table = self.input_table.select(
                    source=pw.this.source,
                    timestamp=pw.this.timestamp,
                    location=pw.this.location,
                    content=pw.this.content,
                    type=pw.this.type,
                    processed_at=pw.now()
                )
                
                print("Pathway streaming pipeline initialized successfully")
                return
            
        except Exception as e:
            print(f"Pathway setup error: {e}")
        
        print("Using manual file monitoring as primary method")
        # Use manual file monitoring as the primary method for this demo
        self.input_table = None
        self.processed_table = None
    
    def embed_text(self, text: str) -> Any:
        """Generate embeddings for text"""
        if self.embedding_model:
            return self.embedding_model.encode(text)
        else:
            # Return a simple hash-based representation as fallback
            return [hash(text) % 1000 for _ in range(384)]  # Mock 384-dim vector
    
    def add_document(self, doc_data: Dict[str, Any]):
        """Add a document to the vector store"""
        content = doc_data.get('content', '')
        if content:
            embedding = self.embed_text(content)
            
            self.documents.append(content)
            self.embeddings.append(embedding)
            self.document_metadata.append(doc_data)
    
    def similarity_search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """Perform similarity search on the vector store"""
        if not self.embeddings:
            return []
        
        # If numpy is not available, do simple keyword matching
        if not np:
            return self.keyword_search(query, top_k)
        
        query_embedding = self.embed_text(query)
        embeddings_array = np.array(self.embeddings)
        
        # Calculate cosine similarity
        similarities = np.dot(embeddings_array, query_embedding) / (
            np.linalg.norm(embeddings_array, axis=1) * np.linalg.norm(query_embedding)
        )
        
        # Get top-k most similar documents
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0.1:  # Minimum similarity threshold
                results.append({
                    'content': self.documents[idx],
                    'metadata': self.document_metadata[idx],
                    'similarity': float(similarities[idx])
                })
        
        return results
    
    def keyword_search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """Fallback keyword-based search when numpy is not available"""
        query_words = set(query.lower().split())
        results = []
        
        for i, (doc, metadata) in enumerate(zip(self.documents, self.document_metadata)):
            doc_words = set(doc.lower().split())
            # Simple word overlap score
            overlap = len(query_words.intersection(doc_words))
            if overlap > 0:
                results.append({
                    'content': doc,
                    'metadata': metadata,
                    'similarity': overlap / len(query_words)  # Normalized overlap
                })
        
        # Sort by similarity and return top-k
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:top_k]
    
    def load_existing_data(self):
        """Load existing data from the data directory"""
        if not os.path.exists(self.data_dir):
            return
        
        for filename in os.listdir(self.data_dir):
            if filename.endswith('.json'):
                filepath = os.path.join(self.data_dir, filename)
                try:
                    with open(filepath, 'r') as f:
                        data = json.load(f)
                        self.add_document(data)
                except Exception as e:
                    print(f"Error loading {filename}: {e}")
    
    def monitor_new_files(self):
        """Monitor for new files and add them to the vector store"""
        import time
        processed_files = set()
        
        while True:
            try:
                if os.path.exists(self.data_dir):
                    current_files = set(os.listdir(self.data_dir))
                    new_files = current_files - processed_files
                    
                    for filename in new_files:
                        if filename.endswith('.json'):
                            filepath = os.path.join(self.data_dir, filename)
                            try:
                                with open(filepath, 'r') as f:
                                    data = json.load(f)
                                    self.add_document(data)
                                    print(f"Added new document: {filename}")
                            except Exception as e:
                                print(f"Error processing new file {filename}: {e}")
                    
                    processed_files = current_files
                
                time.sleep(5)  # Check every 5 seconds
                
            except Exception as e:
                print(f"Error monitoring files: {e}")
                time.sleep(10)
    
    async def call_llm(self, prompt: str) -> str:
        """Call LLM via OpenRouter API using OpenAI client"""
        try:
            if not self.openai_client:
                # Use simulation instead of returning error
                print("⚠️ No API client available, using simulation mode")
                return self.simulate_llm_response(prompt)
            
            # Verify API key is still valid before making request
            if not self.openrouter_api_key or self.openrouter_api_key == "your_openrouter_api_key_here":
                print("⚠️ Invalid API key detected, using simulation mode")
                return self.simulate_llm_response(prompt)
            
            print(f"🤖 Calling OpenRouter API with model: {self.model_name}")
            completion = self.openai_client.chat.completions.create(
                extra_headers={
                    "HTTP-Referer": "https://localhost:8000",  # Update with actual domain
                    "X-Title": "Live Insurance Risk Assessment",     # Site title for rankings
                },
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You are an expert insurance underwriter. Analyze the data and distinguish between real news incidents and test/simulated alerts. Provide accurate risk assessments based on actual events only."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,  # Lower temperature for more consistent results
                max_tokens=800    # Increased for more detailed responses
            )
            
            response = completion.choices[0].message.content
            print(f"✅ OpenRouter API response received ({len(response)} chars)")
            return response
            
        except Exception as e:
            error_msg = str(e)
            print(f"🚨 LLM API error: {e}")
            
            # Check if it's a 401 authentication error
            if "401" in error_msg or "auth" in error_msg.lower() or "unauthorized" in error_msg.lower():
                print("🔄 API authentication failed, falling back to simulation mode")
                return self.simulate_llm_response(prompt)
            
            # For other errors, still fall back to simulation
            print("🔄 API error occurred, falling back to simulation mode")
            return self.simulate_llm_response(prompt)
    
    def simulate_llm_response(self, prompt: str) -> str:
        """Context-aware simulation that considers both real and demo incidents"""
        # Extract address from prompt for personalized response
        import re
        address_match = re.search(r'PROPERTY: ([^\n]+)', prompt)
        address = address_match.group(1) if address_match else "the specified location"
        
        # Look for the unified incidents section in the new prompt format
        prompt_lower = prompt.lower()
        incidents_section = ""
        
        if "current incidents" in prompt_lower:
            incidents_start = prompt.find("CURRENT INCIDENTS")
            if incidents_start != -1:
                # Find the end of the incidents section
                next_section_patterns = ["Assessment Instructions:", "Assessment Requirements:", "Provide a comprehensive"]
                end_pos = len(prompt)
                for pattern in next_section_patterns:
                    pattern_pos = prompt.find(pattern, incidents_start)
                    if pattern_pos != -1:
                        end_pos = min(end_pos, pattern_pos)
                incidents_section = prompt[incidents_start:end_pos]
        
        # Count different types of incidents
        fire_incidents = incidents_section.lower().count("fire")
        flood_incidents = incidents_section.lower().count("flood") 
        crime_incidents = incidents_section.lower().count("crime") + incidents_section.lower().count("theft") + incidents_section.lower().count("larceny")
        earthquake_incidents = incidents_section.lower().count("earthquake")
        
        total_incidents = fire_incidents + flood_incidents + crime_incidents + earthquake_incidents
        
        print(f"🤖 Simulation mode analysis:")
        print(f"   - Fire incidents: {fire_incidents}")
        print(f"   - Flood incidents: {flood_incidents}")
        print(f"   - Crime incidents: {crime_incidents}")
        print(f"   - Earthquake incidents: {earthquake_incidents}")
        print(f"   - Total incidents: {total_incidents}")
        
        # Calculate risk based on incident types and count
        base_risk = 3
        risk_modifiers = 0
        risk_factors = []
        
        if fire_incidents > 0:
            risk_modifiers += 2 * fire_incidents
            risk_factors.append(f"Fire incidents detected ({fire_incidents})")
            
        if flood_incidents > 0:
            risk_modifiers += 2 * flood_incidents  
            risk_factors.append(f"Flood incidents detected ({flood_incidents})")
            
        if earthquake_incidents > 0:
            risk_modifiers += 3 * earthquake_incidents
            risk_factors.append(f"Earthquake incidents detected ({earthquake_incidents})")
            
        if crime_incidents > 0:
            risk_modifiers += 1 * crime_incidents
            risk_factors.append(f"Criminal activity detected ({crime_incidents})")
        
        final_risk = min(10, max(1, base_risk + risk_modifiers))
        
        # Calculate premium based on risk
        base_premium = 500
        premium = base_premium * (1 + 0.1 * final_risk)
        
        # Generate appropriate response based on risk level
        if final_risk <= 3:
            risk_level = "LOW"
            status_emoji = "✅"
            assessment_type = "SAFE AREA"
        elif final_risk <= 6:
            risk_level = "MODERATE" 
            status_emoji = "⚠️"
            assessment_type = "MODERATE RISK"
        else:
            risk_level = "HIGH"
            status_emoji = "🚨"
            assessment_type = "HIGH RISK"
            
        risk_details = risk_factors if risk_factors else ["No significant incidents detected", "Standard area risk profile"]
        
        print(f"� Calculated risk: {final_risk}/10 ({risk_level}) - Premium: ${premium:.0f}")
        
        return f"""**{status_emoji} {assessment_type} ASSESSMENT for {address}**

**Risk Summary:** {'Multiple risk factors detected requiring increased coverage.' if total_incidents > 1 else 'Risk assessment based on current incident data and location factors.' if total_incidents == 1 else 'Standard risk assessment with no major incidents detected.'}

**Risk Score:** {final_risk}/10

**Insurance Quote:** ${premium:.0f}/month for $1M coverage

**Assessment Details:**
{chr(10).join(f'- {factor}' for factor in risk_details)}
- Location-based risk factors considered
- {'Emergency response protocols recommended' if final_risk > 6 else 'Standard monitoring protocols' if final_risk > 3 else 'Routine coverage requirements'}

**Status:** {'Elevated risk area requiring enhanced coverage' if final_risk > 6 else 'Moderate risk area with standard coverage' if final_risk > 3 else 'Safe area with normal risk levels'}"""
    
    async def query_rag(self, address: str, query: str) -> Dict[str, Any]:
        """Main RAG query function with location-specific analysis"""
        
        # Get location-specific risk factors
        location_factors = self.location_analyzer.analyze_location_risk_factors(address)
        
        # Search for relevant documents
        search_query = f"{address} {query}"
        relevant_docs = self.similarity_search(search_query, top_k=5)
        
        # Build context from relevant documents
        context_parts = []
        for i, doc in enumerate(relevant_docs, 1):
            context_parts.append(f"Document {i}:\n{doc['content']}\n")
        
        context = "\n---\n".join(context_parts) if context_parts else "No relevant incidents found in recent data."
        
        # Build enhanced prompt with location context
        location_context = self.build_location_context(location_factors)
        
        # Analyze the data sources to distinguish between real and test alerts
        real_incidents = []
        test_incidents = []
        
        # Extract key location identifiers from the address for filtering
        address_lower = address.lower()
        address_parts = set()
        
        # Extract city, state, street names, etc.
        import re
        # Common patterns: "Street Name, City, State" or "Number Street Name, City, State"
        if ',' in address:
            parts = [part.strip() for part in address.split(',')]
            # Use more specific parts (avoid generic words)
            for part in parts:
                if len(part.strip()) > 4 and part.strip().lower() not in ['street', 'avenue', 'lane', 'road', 'dr', 'ave']:
                    address_parts.add(part.strip().lower())
        
        # Extract street name and number more precisely
        street_match = re.match(r'(\d+)\s+(.+?)(?:\s+(?:st|street|ave|avenue|lane|ln|road|rd|dr|drive))?(?:\s*,|$)', address, re.IGNORECASE)
        if street_match:
            number, street_name = street_match.groups()
            if len(street_name.strip()) > 3:
                address_parts.add(street_name.strip().lower())
            # Only use street number if it's unique enough (3+ digits)
            if len(number) >= 3:
                address_parts.add(number)
        
        # Remove generic words that cause false matches
        generic_words = {'the', 'and', 'of', 'in', 'on', 'at', 'to', 'for', 'with', 'by', 'street', 'avenue', 'lane', 'road', 'drive'}
        address_parts = {part for part in address_parts if part not in generic_words and len(part) > 2}
        
        print(f"🔍 Address filtering for: {address}")
        print(f"    Looking for specific parts: {address_parts}")
        
        for doc in relevant_docs:
            source = doc['metadata'].get('source', '')
            content = doc['content']
            is_demo = doc['metadata'].get('is_demo', False)
            doc_location = doc['metadata'].get('location', '').lower()
            
            # Check if this incident is relevant to the specific address
            is_location_relevant = False
            
            if is_demo:
                # For demo alerts, check if the content mentions the specific address
                content_lower = content.lower()
                if any(part in content_lower for part in address_parts if len(part) > 3):
                    is_location_relevant = True
                    print(f"📋 Demo incident matches address: {content[:50]}...")
                else:
                    print(f"🚫 Demo incident filtered out (different location): {content[:50]}...")
                    continue
            else:
                # For real incidents, use broader location matching (same city/area)
                if any(part in doc_location for part in address_parts if len(part) > 3):
                    is_location_relevant = True
                elif any(part in content.lower() for part in address_parts if len(part) > 4):
                    is_location_relevant = True
                else:
                    print(f"🚫 Real incident filtered out (different location): {content[:50]}...")
                    continue
            
            # More robust demo/test detection
            demo_indicators = ['manual_injection', 'test', 'simulated', 'demo_test', 'demo_news', 'demo']
            content_demo_indicators = ['DEMO:', 'TEST:', 'SIMULATED:']
            
            # Check multiple indicators for demo content
            is_demo_content = (
                is_demo or  # Explicit demo flag
                any(indicator in source.lower() for indicator in demo_indicators) or
                any(indicator in content for indicator in content_demo_indicators) or
                source == 'demo_test'  # Explicit demo source
            )
            
            if is_demo_content:
                test_incidents.append(f"DEMO/TEST ALERT: {content}")
                print(f"📋 Classified as DEMO: {source} - {content[:50]}...")
            elif source in ['newsdata_io', 'nyc_open_data', 'chicago_open_data', 'usgs_earthquake', 'nws_weather']:
                real_incidents.append(f"REAL INCIDENT: {content}")
                print(f"🚨 Classified as REAL: {source} - {content[:50]}...")
            else:
                # For unknown source, be more conservative - default to real unless clearly demo
                if any(indicator in content.lower() for indicator in ['demo', 'test', 'simulated']):
                    test_incidents.append(f"DEMO ALERT: {content}")
                    print(f"📋 Classified as DEMO (content-based): {content[:50]}...")
                else:
                    real_incidents.append(f"INCIDENT: {content}")
                    print(f"⚠️ Classified as REAL (unknown source): {source} - {content[:50]}...")
        
        # Combine real and test incidents for a unified risk assessment
        all_incidents = real_incidents + test_incidents
        
        # Clean up prefixes for a unified view, making them all "ALERT"
        all_incidents_cleaned = []
        for incident in all_incidents:
            # Remove existing prefixes and add a standard one
            content = incident.split(": ", 1)[-1]
            all_incidents_cleaned.append(f"ALERT: {content}")

        incident_context = "\n".join(all_incidents_cleaned) if all_incidents_cleaned else "No recent incidents detected in the area. Location appears stable."

        print(f"📊 Analysis summary: {len(real_incidents)} real incidents, {len(test_incidents)} demo alerts. Combined for assessment.")

        # A single, unified prompt that treats all incidents (real or demo) as valid for risk calculation
        prompt = f"""You are an AI Insurance Underwriting Assistant analyzing property risk.

PROPERTY: {address}

LOCATION ANALYSIS:
{location_context}

CURRENT INCIDENTS (includes real-time data and active demo events):
{incident_context}

Assessment Instructions:
- Base your assessment on ALL available data: real-world incidents, active demo events, and location factors.
- Treat active demo events (e.g., fire, flood) as if they are real events for this risk assessment. Their presence signifies an elevated risk scenario.
- The presence of multiple incidents (real or demo) should cumulatively increase the perceived risk and the final risk score.
- If no incidents are present, assess risk based on location factors alone.

Provide a comprehensive risk assessment including:
1. A risk summary that clearly mentions the active demo events (if any) and their impact on the assessment.
2. A risk score from 1 (safe) to 10 (critical) based on the combined impact of all real and demo incidents.
3. A monthly insurance quote for $1M coverage, calculated from the final risk score.

Base calculation: ${self.base_cost} × (1 + {self.risk_multiplier} × risk_score)"""
        
        # Get LLM response
        llm_response = await self.call_llm(prompt)
        
        # Extract risk score from response with location consideration
        risk_score = self.extract_risk_score_enhanced(llm_response, location_factors)
        
        # Calculate quote with location factors
        quote = self.calculate_enhanced_quote(risk_score, location_factors)
        
        return {
            'risk_summary': llm_response,
            'risk_score': risk_score,
            'insurance_quote': round(quote, 2),
            'relevant_documents': len(relevant_docs),
            'location_factors': location_factors,
            'timestamp': datetime.now().isoformat()
        }
    
    def extract_risk_score(self, response: str) -> int:
        """Extract risk score from LLM response"""
        import re
        
        # Look for patterns like "Risk Score: 7/10", "7 out of 10", or "8.5"
        patterns = [
            r'Risk Score:?\s*(\d+(?:\.\d+)?)(?:/10)?',
            r'risk score:?\s*(\d+(?:\.\d+)?)(?:/10)?',
            r'Score:?\s*(\d+(?:\.\d+)?)(?:/10)?',
            r'(\d+(?:\.\d+)?)\s*(?:out of|/)\s*10'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, response, re.IGNORECASE)
            if match:
                score = float(match.group(1))
                return min(max(int(round(score)), 1), 10)  # Round and clamp between 1-10
        
        # Default to moderate risk if no score found
        return 5
    
    def build_location_context(self, location_factors: Dict[str, Any]) -> str:
        """Build location context string for LLM prompt"""
        context_parts = []
        
        if location_factors['latitude'] and location_factors['longitude']:
            context_parts.append(f"Coordinates: {location_factors['latitude']:.2f}, {location_factors['longitude']:.2f}")
        
        context_parts.append(f"Area Type: {location_factors['location_description']}")
        context_parts.append(f"Base Risk Level: {location_factors['base_risk_score']:.1f}/10")
        
        if location_factors['primary_risks']:
            context_parts.append(f"Primary Regional Risks: {', '.join(location_factors['primary_risks'])}")
        
        if location_factors['risk_multipliers']:
            multiplier_info = []
            for risk_type, multiplier in location_factors['risk_multipliers'].items():
                multiplier_info.append(f"{risk_type} ({multiplier:.1f}x)")
            context_parts.append(f"Risk Multipliers: {', '.join(multiplier_info)}")
        
        return "\n".join(context_parts)
    
    def extract_risk_score_enhanced(self, llm_response: str, location_factors: Dict[str, Any]) -> int:
        """Enhanced risk score extraction with location consideration"""
        # Check if this is an error message
        if "❌" in llm_response or "Error" in llm_response or "API Error" in llm_response:
            # Return base location risk for errors, not a default high score
            return max(1, int(location_factors.get('base_risk_score', 3)))
        
        # Try to extract from LLM response first
        base_score = self.extract_risk_score(llm_response)
        
        # If using default score, consider location factors
        if base_score == 5:  # Default fallback score
            base_score = max(2, location_factors['base_risk_score'])
        
        # Ensure score is within bounds
        return max(1, min(10, int(base_score)))
    
    def calculate_enhanced_quote(self, risk_score: int, location_factors: Dict[str, Any]) -> float:
        """Calculate quote with location-specific adjustments"""
        base_quote = self.base_cost * (1 + self.risk_multiplier * risk_score)
        
        # Apply location-specific multipliers (dampened to avoid extreme quotes)
        total_multiplier = 1.0
        for risk_type, multiplier in location_factors.get('risk_multipliers', {}).items():
            # Apply a dampened version of the multiplier to avoid extreme quotes
            dampened_multiplier = 1 + (multiplier - 1) * 0.25  # 25% of the full multiplier effect
            total_multiplier *= dampened_multiplier
        
        return base_quote * total_multiplier

    def clear_document_store(self):
        """Clear the in-memory document store"""
        self.documents = []
        self.embeddings = []
        self.document_metadata = []
        print("🧹 Cleared in-memory document store")

if __name__ == "__main__":
    from dotenv import load_dotenv
    import threading
    
    load_dotenv()
    
    # Initialize pipeline
    pipeline = LiveRAGPipeline()
    
    # Load existing data
    pipeline.load_existing_data()
    
    # Start file monitoring in background
    monitor_thread = threading.Thread(target=pipeline.monitor_new_files, daemon=True)
    monitor_thread.start()
    
    print("RAG Pipeline initialized and monitoring for new data...")
    
    # Keep alive
    try:
        while True:
            import time
            time.sleep(1)
    except KeyboardInterrupt:
        print("Stopping RAG pipeline...")
